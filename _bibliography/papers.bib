---
---

@string{aps = {American Physical Society,}}



@misc{liu2024datasetdistillationwassersteinmetric,
      title={Dataset Distillation via the Wasserstein Metric}, 
      author={H. Liu and Y. Li and T. Xing and V. Dalal and L. Li and J. He and H. Wang},
      year={2024},
      eprint={2311.18531},
      abstract={Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a range of high-resolution datasets. Extensive testing demonstrates the effectiveness and adaptability of our method, underscoring the untapped potential of Wasserstein metrics in dataset distillation.},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      bibtex_show={true},
      url={https://arxiv.org/abs/2311.18531}, 
      selected={true},
      arxiv={2311.18531},
      preview={wasss.png}
}

@misc{dalal2024shorttimefouriertransformdeblurring,
      title={Short-Time Fourier Transform for deblurring Variational Autoencoders}, 
      author={V. Dalal},
      year={2024},
      eprint={2401.03166},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      bibtex_show={true},
      url={https://arxiv.org/abs/2401.03166}, 
      abstract={Variational Autoencoders (VAEs) are powerful generative models, however their generated samples are known to suffer from a characteristic blurriness, as compared to the outputs of alternative generating techniques. Extensive research efforts have been made to tackle this problem, and several works have focused on modifying the reconstruction term of the evidence lower bound (ELBO). In particular, many have experimented with augmenting the reconstruction loss with losses in the frequency domain. Such loss functions usually employ the Fourier transform to explicitly penalise the lack of higher frequency components in the generated samples, which are responsible for sharp visual features. In this paper, we explore the aspects of previous such approaches which aren't well understood, and we propose an augmentation to the reconstruction term in response to them. Our reasoning leads us to use the short-time Fourier transform and to emphasise on local phase coherence between the input and output samples. We illustrate the potential of our proposed loss on the MNIST dataset by providing both qualitative and quantitative results.},
      selected={true},
      arxiv={2401.03166},
      preview={vae3.png}
}

@article{dabouei2024deep,
  title={Deep Video Anomaly Detection in Automated Laboratory Setting},
  author={A. Dabouei* and JP. Shibu* and V. Dalal* and C. Cao and MacWilliams, Andy and Kangas, Joshua and Xu, Min},
  year={2024},
  journal={SSRN},
  bibtex_show={true},
  abstract={Laboratory automation integrates robotics, machine learning, and computer vision to enhance precision and efficiency while reducing costs. Despite its pivotal importance in fully automated experimentation, automatic monitoring of procedures is an overlooked task. In this paper, we aim to address this shortcoming by developing a learning method for detecting anomalies in the laboratory setting. To formalize the problem, we focus on the liquid transfer task as a major task in laboratory automation, given the frequent need for reagent mixing and solution preparation. We introduce a novel video anomaly detection framework that leverages the robust CLIP features in conjunction with a transformer encoder. Through an array of experiments and ablation studies, we demonstrate that our proposed method surpasses current state-of-the-art anomaly detection techniques by a notable margin of over 11%, achieving an impressive AUC of 98.79% for video-level anomaly detection.},
  selected={true},
  html={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4887151},
  preview={anomaly.png}
}


